[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Report Proyek Sain Data",
    "section": "",
    "text": "Introduction\n\n\nProyek Sains Data\nProyek Sains Data adalah rangkaian kegiatan yang bertujuan untuk menerapkan metode sains data guna memahami, menganalisis, dan mengekstraksi pengetahuan atau informasi berharga dari data. Proyek-proyek ini seringkali melibatkan beberapa tahapan, mulai dari pengumpulan data, pembersihan data, analisis, hingga pembuatan model prediktif yang dapat digunakan untuk membuat keputusan atau mengidentifikasi pola.\n\nlangkah-langkah umum dalam Proyek Sains Data:\n\n1. Pemahaman Bisnis (Business Understanding):\n\nMeng-Identifikasi tujuan bisnis dan masalah yang ingin dipecahkan dan Memahami konteks bisnis, konstrain, dan kebutuhan pemangku kepentingan.\n\n\n\n2. Pengumpulan Data:\n\nMeng-Identifikasi dan kumpulkan data yang relevan dengan permasalahan. Data dapat berasal dari berbagai sumber, termasuk database internal, sumber eksternal, atau bahkan data yang dihasilkan oleh sensor atau perangkat IoT.\n\n\n\n3. Pembersihan Data (Data Cleaning):\n\nMeng-Identifikasi dan menangangani missing values (nilai yang hilang) dan outliers (data yang ekstrim atau tidak biasa).\n\n\n\n4. Eksplorasi Data (Data Exploration):\n\nMeng-Analisa statistik deskriptif\n\n\n\n5. Pemodelan Data:\n\nMemilih model atau algoritma yang sesuai kemudian melatih model dengan menggunakan data lati, lalu Evaluasi model menggunakan data uji.\n\n\n\n6. Implementasi:\n\nMeng-Implementasikan solusi berdasarkan temuan atau model yang dihasilkan. Me-Monitor performa solusi dan lakukan perbaikan jika diperlukan."
  },
  {
    "objectID": "PSD_Je.html",
    "href": "PSD_Je.html",
    "title": "1¬† Tic-Tac-Toe EndGame (Memprediksi apakah permainan akan dimenangkan oleh pemain ‚Äòx‚Äô atau tidak)",
    "section": "",
    "text": "2 Bussiness Understanding\nTUJUAN ANALISIS DATA : Analisis data Tic-TacToe EndGame ini untuk memprediksi apakah permainan akan dimenangkan oleh pemain ‚Äòx‚Äô atau tidak. ‚Äòwin for x,‚Äô yang berarti pemain ‚Äòx‚Äô harus berhasil menciptakan salah satu dari 8 kemungkinan cara untuk mencapai ‚Äòthree-in-a-row,‚Äô yaitu menempatkan tiga tanda ‚Äòx‚Äô secara berurutan dalam satu baris, kolom, atau diagonal.\nKarena Dataset bertipe Categorical maka akan diubah menjadi numerik menggunakan Label Encoding\nLabel encoding adalah suatu metode dalam pra-pemrosesan data yang melibatkan penggantian nilai-nilai kategori pada suatu fitur dengan nilai-nilai numerik yang unik.\nMengubah categorical menjadi numerik menggunakan Label Encoding dilakukan karena sebagian besar algoritma machine learning memerlukan input yang bersifat numerik. Beberapa algoritma, terutama yang berbasis pada perhitungan jarak (seperti k-Nearest Neighbors), dapat memberikan hasil yang lebih baik jika nilai kategorikal diubah menjadi bentuk numerik. Hal ini karena perhitungan jarak lebih mudah dilakukan pada data numerik.\ncatatan : Karena fitur-fitur ini berada dalam rentang yang sangat terbatas (hanya 1, 0, dan -1), normalisasi tidak diperlukan. Normalisasi biasanya digunakan pada data numerik yang memiliki rentang nilai yang berbeda agar fitur-fitur tersebut memiliki skala yang serupa.\n# Memisahkan fitur (X) dan target (y)\nX = dataset.drop('Class', axis=1)\ny = dataset['Class']\n\n# Membagi data menjadi data latih dan data uji\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=670, test_size=288, random_state=42)\n# Mengimpor data CSV tanpa nama kolom (header)\ndata = pd.read_csv(url, header=None)\n\n# Memberikan kolom pada dataset\ncolumn_names = [\"top-left-square\", \"top-middle-square\", \"top-right-square\", \"middle-left-square\", \"middle-middle-square\", \"middle-right-square\", \"bottom-left-square\", \"bottom-middle-square\", \"bottom-right-square\", \"Class\"]\ndata.columns = column_names\n\n# Mapping karakter ke angka (label encoding)\nchar_to_num = {'x': 1, 'o': 0, 'b': -1}\n# Melakukan encoding untuk data train\nencoded_data_train = []\n\n# Mengonversi setiap baris dalam DataFrame X_train menjadi karakter dan menyimpannya dalam encoded_data_train\nfor index, row in X_train.iterrows():\n    encoded_row = [char_to_num[c] for c in row]\n    encoded_data_train.append(encoded_row)\n\n# Membuat DataFrame dari data train yang telah diencode\nencoded_df_train = pd.DataFrame(encoded_data_train, columns=X_train.columns)\n\n# Menampilkan DataFrame data train yang sudah diencode\nprint(\"Data Train yang sudah diencode:\")\nprint(encoded_df_train)\n\nData Train yang sudah diencode:\n     top-left-square  top-middle-square  top-right-square  middle-left-square  \\\n0                  1                 -1                 1                   0   \n1                  0                  1                 0                   1   \n2                  1                 -1                 1                  -1   \n3                  1                  0                -1                   0   \n4                  1                  1                 0                   1   \n..               ...                ...               ...                 ...   \n665                1                  1                -1                   0   \n666                1                 -1                 0                  -1   \n667                0                 -1                 1                   0   \n668                0                 -1                 1                   0   \n669                1                  1                -1                   1   \n\n     middle-middle-square  middle-right-square  bottom-left-square  \\\n0                       1                    0                   1   \n1                       1                    1                   0   \n2                       1                   -1                   0   \n3                       1                   -1                   0   \n4                       0                    1                   1   \n..                    ...                  ...                 ...   \n665                     1                    0                   0   \n666                     1                    0                   0   \n667                     1                   -1                   0   \n668                     1                    0                   1   \n669                     0                    0                   1   \n\n     bottom-middle-square  bottom-right-square  \n0                      -1                    0  \n1                       1                    0  \n2                       0                    0  \n3                       1                    1  \n4                       0                    0  \n..                    ...                  ...  \n665                    -1                    1  \n666                     1                    1  \n667                    -1                    1  \n668                    -1                    1  \n669                    -1                    0  \n\n[670 rows x 9 columns]\n# Melakukan encoding untuk data test\nencoded_data_test = []\n\n# Mengonversi setiap baris dalam DataFrame X_train menjadi karakter dan menyimpannya dalam encoded_data_train\nfor index, row in X_test.iterrows():\n    encoded_row = [char_to_num[c] for c in row]\n    encoded_data_test.append(encoded_row)\n\n# Membuat DataFrame dari data test yang telah diencode\nencoded_df_test = pd.DataFrame(encoded_data_test, columns=X_test.columns)\n\n# Menampilkan DataFrame data test yang sudah diencode\nprint(\"Data Test yang sudah diencode:\")\nprint(encoded_df_test)\n\nData Test yang sudah diencode:\n     top-left-square  top-middle-square  top-right-square  middle-left-square  \\\n0                  0                  0                 0                  -1   \n1                  0                 -1                -1                   1   \n2                  0                  1                -1                   1   \n3                 -1                  1                 0                   1   \n4                  0                 -1                 1                   1   \n..               ...                ...               ...                 ...   \n283               -1                  1                 0                   0   \n284                0                  1                -1                   0   \n285                1                  1                 0                   1   \n286                0                  0                 1                  -1   \n287                0                  1                 1                   1   \n\n     middle-middle-square  middle-right-square  bottom-left-square  \\\n0                       1                    1                   1   \n1                       1                    1                  -1   \n2                       1                   -1                   0   \n3                       1                    0                   1   \n4                       0                   -1                  -1   \n..                    ...                  ...                 ...   \n283                     1                    1                   0   \n284                     0                    1                   0   \n285                     0                   -1                   0   \n286                    -1                    1                   0   \n287                     0                    1                   0   \n\n     bottom-middle-square  bottom-right-square  \n0                       0                    1  \n1                      -1                    0  \n2                       1                    0  \n3                       0                    0  \n4                       1                    0  \n..                    ...                  ...  \n283                     1                   -1  \n284                     1                    1  \n285                     1                    0  \n286                     1                    1  \n287                    -1                    0  \n\n[288 rows x 9 columns]\nmenyimpan dataset yang telah di encoding ke csv dan pickle\nimport pickle\n\n# Menyimpan DataFrame data train dan data test yang sudah diencode ke file CSV\nencoded_df_train.to_csv(\"encoded_data_train.csv\", index=False)\nencoded_df_test.to_csv(\"encoded_data_test.csv\", index=False)\n\n# Menyimpan data train yang sudah diencode ke dalam file pickle\nwith open('encoded_data_train.pkl', 'wb') as file:\n    pickle.dump(encoded_df_train, file)\n\n# Menyimpan data test yang sudah diencode ke dalam file pickle\nwith open('encoded_data_test.pkl', 'wb') as file:\n    pickle.dump(encoded_df_test, file)\n\n# print(\"Data yang sudah diencoding telah disimpan ke CSV\")\n# Membaca data train yang sudah diencode dari file pickle\nwith open('encoded_data_train.pkl', 'rb') as file:\n    loaded_encoded_df_train = pickle.load(file)\n\n# Membaca data test yang sudah diencode dari file pickle\nwith open('encoded_data_test.pkl', 'rb') as file:\n    loaded_encoded_df_test = pickle.load(file)\n# Membangun model K-NN\n# Hitung akurasi KNN dari k = 1 hingga 30\nimport numpy as np\n\nk = 30\nacc = np.zeros((k - 1))\n\nfor n in range(1, k, 2):\n    knn = KNeighborsClassifier(n_neighbors=n, metric=\"euclidean\").fit(loaded_encoded_df_train, y_train)\n    y_pred = knn.predict(loaded_encoded_df_test)\n    acc[n - 1] = accuracy_score(y_test, y_pred)\n\nbest_accuracy = acc.max()\nbest_k = acc.argmax() + 1\n\nprint('Akurasi KNN terbaik adalah', best_accuracy, 'dengan nilai k =', best_k)\n\nresults_knn = pd.DataFrame({'Actual Label': y_test, 'Prediksi': y_pred})\nresults_knn\n\nAkurasi KNN terbaik adalah 0.8472222222222222 dengan nilai k = 5\n\n\n\n  \n    \n\n\n\n\n\n\nActual Label\nPrediksi\n\n\n\n\n836\nnegative\npositive\n\n\n477\npositive\npositive\n\n\n350\npositive\npositive\n\n\n891\nnegative\nnegative\n\n\n855\nnegative\nnegative\n\n\n...\n...\n...\n\n\n501\npositive\npositive\n\n\n796\nnegative\nnegative\n\n\n634\nnegative\npositive\n\n\n405\npositive\npositive\n\n\n741\nnegative\nnegative\n\n\n\n\n\n288 rows √ó 2 columns\n# Inisialisasi model Naive Bayes Multinomial\nnb = GaussianNB()\nnb.fit(loaded_encoded_df_train, y_train)\n\n# Lakukan prediksi pada data uji\ny_pred = nb.predict(loaded_encoded_df_test)\n\n# Evaluasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi: {accuracy:.2f}')\n\nresults_nb = pd.DataFrame({'Actual Label': y_test, 'Prediksi': y_pred})\nresults_nb\n\nAkurasi: 0.75\n\n\n\n  \n    \n\n\n\n\n\n\nActual Label\nPrediksi\n\n\n\n\n836\nnegative\npositive\n\n\n477\npositive\npositive\n\n\n350\npositive\npositive\n\n\n891\nnegative\npositive\n\n\n855\nnegative\npositive\n\n\n...\n...\n...\n\n\n501\npositive\npositive\n\n\n796\nnegative\nnegative\n\n\n634\nnegative\nnegative\n\n\n405\npositive\npositive\n\n\n741\nnegative\nnegative\n\n\n\n\n\n288 rows √ó 2 columns\n# Inisialisasi model Logistic Regression\nlogreg_model = LogisticRegression()\n\n# Melatih model menggunakan data train yang sudah diencode\nlogreg_model.fit(loaded_encoded_df_train, y_train)\n\n# Membuat prediksi menggunakan data test yang sudah diencode\ny_pred = logreg_model.predict(loaded_encoded_df_test)\n\n# Evaluasi model\naccuracy = accuracy_score(y_test, y_pred)\nclassification_rep = classification_report(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy}')\nresults_lr = pd.DataFrame({'Actual Label': y_test, 'Prediksi': y_pred})\nresults_lr\n\nAccuracy: 0.6909722222222222\n\n\n\n  \n    \n\n\n\n\n\n\nActual Label\nPrediksi\n\n\n\n\n836\nnegative\npositive\n\n\n477\npositive\npositive\n\n\n350\npositive\npositive\n\n\n891\nnegative\npositive\n\n\n855\nnegative\npositive\n\n\n...\n...\n...\n\n\n501\npositive\npositive\n\n\n796\nnegative\nnegative\n\n\n634\nnegative\npositive\n\n\n405\npositive\npositive\n\n\n741\nnegative\npositive\n\n\n\n\n\n288 rows √ó 2 columns\nDecision tree adalah model prediktif dalam analisis data yang menggunakan struktur pohon untuk menentukan keputusan . Decision tree sering digunakan dalam klasifikasi dan regresi. Decision tree dengan teknik regresi yang paling terkenal yaitu CART (Classification and Regression Tree) yang diperkenalkan oleh Professor Breimann.\nAda beberapa tahapan yang harus dilakukan untuk membuat sebuah pohon keputusan, yaitu:\n\\(Entropy (S) = \\displaystyle\\sum_{i=1}^{n} - ùëùi . log_2 pi\\)\nKeterangan:\nS = himpunan kasus\nn = jumlah partisi S\npi = proporsi Si terhadap S\n\\(Gain (S, A) = Entropy(S) - \\displaystyle\\sum_{i=1}^{n} \\frac{|Si|}{|S|} * Entropy (Si)\\)\nKeterangan:\nS = himpunan kasus\nA = fitur\nn = jumlah partisi atribut A\n|Si| = proporsi Si terhadap S\n|S| = jumlah kasus dalam S\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Membangun model Decision Tree\ndt = DecisionTreeClassifier()\ndt.fit(loaded_encoded_df_train, y_train)\n\n# Melakukan prediksi\ny_pred = dt.predict(loaded_encoded_df_test)\n\n# Evaluasi model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Akurasi: {accuracy:.2f}')\n\n\nresults_compare = pd.DataFrame({'Actual Label': y_test, 'Prediksi': y_pred})\nresults_compare\n\nAkurasi: 0.92\n\n\n\n  \n    \n\n\n\n\n\n\nActual Label\nPrediksi\n\n\n\n\n836\nnegative\nnegative\n\n\n477\npositive\npositive\n\n\n350\npositive\npositive\n\n\n891\nnegative\nnegative\n\n\n855\nnegative\nnegative\n\n\n...\n...\n...\n\n\n501\npositive\npositive\n\n\n796\nnegative\nnegative\n\n\n634\nnegative\nnegative\n\n\n405\npositive\npositive\n\n\n741\nnegative\nnegative\n\n\n\n\n\n288 rows √ó 2 columns\n# Membangun model Decision Tree\ndt = DecisionTreeClassifier()\ndt.fit(loaded_encoded_df_train, y_train)\n\n# Input data untuk prediksi\nuser_inputs = {}\nfor feature in loaded_encoded_df_train.columns:\n    user_input = input(f\"Masukkan nilai untuk '{feature}': \")\n    user_inputs[feature] = char_to_num[user_input]\n\n# Membuat DataFrame dari input pengguna\nnew_data = pd.DataFrame([user_inputs])\n\n# Melakukan prediksi CLASS untuk data baru menggunakan model yang dimuat\nprediction = dt.predict(new_data)\n\n# Menampilkan hasil prediksi\npredicted_class = prediction[0]  # Menggunakan indeks 0 karena kita hanya memprediksi satu data\n\n# Menampilkan hasil prediksi\nif predicted_class == 'positive':\n    print(\"Hasil Prediksi: Positif (Menang)\")\nelse:\n    print(\"Hasil Prediksi: Negatif (Kalah)\")\n\nMasukkan nilai untuk 'top-left-square': x\nMasukkan nilai untuk 'top-middle-square': o\nMasukkan nilai untuk 'top-right-square': b\nMasukkan nilai untuk 'middle-left-square': b\nMasukkan nilai untuk 'middle-middle-square': x\nMasukkan nilai untuk 'middle-right-square': o\nMasukkan nilai untuk 'bottom-left-square': x\nMasukkan nilai untuk 'bottom-middle-square': o\nMasukkan nilai untuk 'bottom-right-square': x\nHasil Prediksi: Positif (Menang)\nDeploy Streamlit"
  },
  {
    "objectID": "PSD_Je.html#penjelasan-dataset",
    "href": "PSD_Je.html#penjelasan-dataset",
    "title": "1¬† Tic-Tac-Toe EndGame (Memprediksi apakah permainan akan dimenangkan oleh pemain ‚Äòx‚Äô atau tidak)",
    "section": "3.1 PENJELASAN DATASET :",
    "text": "3.1 PENJELASAN DATASET :\nDataset ini merupakan kumpulan lengkap konfigurasi papan di akhir permainan tic-tac-toe, di mana ‚Äòx‚Äô diasumsikan bermain terlebih dahulu. Konsep targetnya adalah ‚Äòkemenangan untuk x‚Äô (membuat ‚Äòtiga berjejer‚Äô).\nDalam permainan Tic-Tac-Toe, ada dua pemain, biasanya disebut ‚Äòx‚Äô dan ‚Äòo,‚Äô yang bergiliran menempatkan tanda mereka di papan permainan 3x3. Tujuan permainan adalah mencapai ‚Äòwin for x,‚Äô yang berarti pemain ‚Äòx‚Äô harus berhasil menciptakan salah satu dari 8 kemungkinan cara untuk mencapai ‚Äòthree-in-a-row,‚Äô yaitu menempatkan tiga tanda ‚Äòx‚Äô secara berurutan dalam satu baris, kolom, atau diagonal.\nDeskripsi tersebut mencatat bahwa Dataset ini mencakup semua konfigurasi papan pada akhir permainan Tic-Tac-Toe, dengan asumsi bahwa ‚Äòx‚Äô selalu bermain pertama.\nIni adalah tugas klasifikasi di mana model pembelajaran mesin mencoba memprediksi hasil permainan, yaitu apakah ‚Äòx‚Äô akan memenangkan permainan atau tidak.\n\nTipe Data &gt; Tipe data dari Dataset ini adalah Categorical\nSubject Area &gt; Dataset ini digunakan dalam Game Tic-Tac-Toe\njumlah data : 958 bari &gt; Dataset ini berisi 958 permainan yang mencakup semua konfigurasi papan pada akhir permainan Tic-Tac-Toe\nMissing Value &gt;Dataset ini TIDAK MEMILIKI Missing Values** yang berarti bahwa setiap bagian data pada dataset lengkap dengan informasi.\n\nDataset ini menggunakan Class sebagai Targetnya dimana Fitur ini menunjukan apakah akhir dari permainan dimenangkan oleh ‚Äòx‚Äô atau tidak.\nDATASET\n\nurl = \"https://raw.githubusercontent.com/jennamacwe/ProyekSainData/main/tic-tac-toe_Endgame2.csv\"\ndataset = pd.read_csv(url, header=None)\n\n# Menambahkan kolom\ndataset.columns = [\"top-left-square\", \"top-middle-square\", \"top-right-square\", \"middle-left-square\", \"middle-middle-square\", \"middle-right-square\", \"bottom-left-square\", \"bottom-middle-square\", \"bottom-right-square\", \"Class\"]\n# Menampilkan dataset dengan kolom tambahan\nprint(dataset)\n\n    top-left-square top-middle-square top-right-square middle-left-square  \\\n0                 x                 x                x                  x   \n1                 x                 x                x                  x   \n2                 x                 x                x                  x   \n3                 x                 x                x                  x   \n4                 x                 x                x                  x   \n..              ...               ...              ...                ...   \n953               o                 x                x                  x   \n954               o                 x                o                  x   \n955               o                 x                o                  x   \n956               o                 x                o                  o   \n957               o                 o                x                  x   \n\n    middle-middle-square middle-right-square bottom-left-square  \\\n0                      o                   o                  x   \n1                      o                   o                  o   \n2                      o                   o                  o   \n3                      o                   o                  o   \n4                      o                   o                  b   \n..                   ...                 ...                ...   \n953                    o                   o                  o   \n954                    x                   o                  x   \n955                    o                   x                  x   \n956                    x                   x                  x   \n957                    x                   o                  o   \n\n    bottom-middle-square bottom-right-square     Class  \n0                      o                   o  positive  \n1                      x                   o  positive  \n2                      o                   x  positive  \n3                      b                   b  positive  \n4                      o                   b  positive  \n..                   ...                 ...       ...  \n953                    x                   x  negative  \n954                    o                   x  negative  \n955                    o                   x  negative  \n956                    o                   x  negative  \n957                    x                   x  negative  \n\n[958 rows x 10 columns]"
  },
  {
    "objectID": "PSD_Je.html#penjelasan-fitur",
    "href": "PSD_Je.html#penjelasan-fitur",
    "title": "1¬† Tic-Tac-Toe EndGame (Memprediksi apakah permainan akan dimenangkan oleh pemain ‚Äòx‚Äô atau tidak)",
    "section": "3.2 PENJELASAN FITUR",
    "text": "3.2 PENJELASAN FITUR\n\n# Menampilkan kolom pada dataset\nprint(dataset.columns)\n\nIndex(['top-left-square', 'top-middle-square', 'top-right-square',\n       'middle-left-square', 'middle-middle-square', 'middle-right-square',\n       'bottom-left-square', 'bottom-middle-square', 'bottom-right-square',\n       'Class'],\n      dtype='object')\n\n\nJumlah Fitur pada dataset Permainan Tic-Tac-toe ini sebanyak 9 Fitur dimana setiap fitur mempresentasikan setiap kotak pada permainan\n\ntop-left-square : kotak pada bagian kiri baris teratas\ntop-middle-square : kotak pada bagian tengah baris teratas\ntop-right-square : kotak pada bagian kanan baris teratas\nmiddle-left-square : kotak pada bagian kiri baris tengah\nmiddle-middle-square : kotak pada bagian tengah baris tengah\nmiddle-right-square : kotak pada bagian kanan baris tengah\nbottom-left-square : kotak pada bagian kiri baris bawah\nbottom-middle-square : kotak pada bagian tengah baris bawah\nbottom-righ-square : kotak pada bagian kiri baris bawah"
  },
  {
    "objectID": "PSD_Je.html#sumber-dataset",
    "href": "PSD_Je.html#sumber-dataset",
    "title": "1¬† Tic-Tac-Toe EndGame (Memprediksi apakah permainan akan dimenangkan oleh pemain ‚Äòx‚Äô atau tidak)",
    "section": "3.3 SUMBER DATASET:",
    "text": "3.3 SUMBER DATASET:\nDataset didapatkan dari UCI Machine Learning Repository dan dapat diakses melalui link di bawah ini:\nhttps://archive.ics.uci.edu/dataset/101/tic+tac+toe+endgam"
  },
  {
    "objectID": "PSD_Je.html#eksplorasi-data",
    "href": "PSD_Je.html#eksplorasi-data",
    "title": "1¬† Tic-Tac-Toe EndGame (Memprediksi apakah permainan akan dimenangkan oleh pemain ‚Äòx‚Äô atau tidak)",
    "section": "3.4 Eksplorasi Data",
    "text": "3.4 Eksplorasi Data\n\n3.4.1 Jumlah per kelas\nMenampilkan grafik jumlah per kelas (negatif dan positif) dari dataset dan Jumlah masing-masing Kelas untuk mengetahui kelas mana yang terbanyak. Jumlah data per Kelas beserta Grafik ditampilkan dibawah ini:\n\n# Menghitung jumlah per kelas\nclass_counts = dataset['Class'].value_counts()\n\n# Print jumlah per kelas\nfor class_label, count in class_counts.items():\n    print(f'Kelas {class_label}: {count} data')\n\nprint(\"\\n\")\n\n# Plot grafik jumlah per kelas\nplt.bar(class_counts.index, class_counts.values)\nplt.xlabel('Kelas')\nplt.ylabel('Jumlah')\nplt.title('Jumlah Per Kelas')\nplt.show()\n\n# Menghitung jumlah per kelas\n# class_counts = dataset['Class'].value_counts()\n\nKelas positive: 626 data\nKelas negative: 332 data\n\n\n\n\n\n\n\n\n\n3.4.2 Statistik ringkasan dari Dataset\nMenampilkan gambaran umum tentang distribusi nilai dalam setiap kolom kategorikal pada dataset.\n\n# Summary statistics\nprint(\"\\nSummary statistics:\")\nprint(dataset.describe(include='all'))\n\n\nSummary statistics:\n       top-left-square top-middle-square top-right-square middle-left-square  \\\ncount              958               958              958                958   \nunique               3                 3                3                  3   \ntop                  x                 x                x                  x   \nfreq               418               378              418                378   \n\n       middle-middle-square middle-right-square bottom-left-square  \\\ncount                   958                 958                958   \nunique                    3                   3                  3   \ntop                       x                   x                  x   \nfreq                    458                 378                418   \n\n       bottom-middle-square bottom-right-square     Class  \ncount                   958                 958       958  \nunique                    3                   3         2  \ntop                       x                   x  positive  \nfreq                    378                 418       626  \n\n\nBerikut adalah tafsir dari beberapa bagian dari output statistik ringkasan:\n\nCount (Jumlah): Menunjukkan jumlah entri non-null (tidak kosong) dalam setiap kolom.\nUnique (Unik): Menunjukkan jumlah nilai unik dalam setiap kolom.\nTop (Teratas): Menunjukkan nilai yang paling sering muncul dalam setiap kolom.\nFreq (Frekuensi): Menunjukkan frekuensi kemunculan nilai teratas dalam setiap kolom.\n\n\n\n3.4.3 Missing Values\nMEnampilkan apakah terdapat nilai yang hilang (missing values) dalam dataset.\nIni adalah langkah yang penting dalam eksplorasi data karena nilai yang hilang dapat mempengaruhi hasil analisis dan model yang dibangun.\n\n# Checking for missing values\nprint(\"Missing values:\")\nprint(dataset.isnull().sum())\n\nMissing values:\ntop-left-square         0\ntop-middle-square       0\ntop-right-square        0\nmiddle-left-square      0\nmiddle-middle-square    0\nmiddle-right-square     0\nbottom-left-square      0\nbottom-middle-square    0\nbottom-right-square     0\nClass                   0\ndtype: int64\n\n\nPada output yang di hasilkan terlihat bahwa tidak ada Missing Values dalam Dataset Permainan Tic-Tac-Toe\n\n\n3.4.4 Distribusi nilai fitur pada dataset\nMenampilkan serangkaian subplot yang masing-masing menunjukkan distribusi nilai untuk setiap fitur dalam dataset. Ini membantu memahami sebaran nilai dan pola distribusi untuk masing-masing kategori atau atribut dalam dataset permainan tic-tac-toe.\n\n# Analisis distribusi nilai fitur\nplt.figure(figsize=(14, 10))  # Membuat gambar dengan ukuran 14x10 inci untuk visualisasi\n\nfor i, feature in enumerate(dataset.columns[:-1], 1):  # Iterasi melalui setiap fitur kecuali kolom terakhir (\"Class\")\n    plt.subplot(3, 3, i)  # Membuat subplot dalam grid 3x3 untuk setiap fitur\n    sns.histplot(dataset[feature], kde=True)  # Menggunakan seaborn untuk membuat histogram fitur dengan plot densitas kernel (kde)\n    plt.title(f\"Distribusi {feature}\")  # Menambahkan judul subplot dengan nama fitur untuk memberikan konteks\n\nplt.tight_layout()  # Mengoptimalkan tata letak subplot agar tidak tumpang tindih\nplt.show()  # Menampilkan visualisasi distribusi nilai fitur"
  },
  {
    "objectID": "Audio.html",
    "href": "Audio.html",
    "title": "2¬† Memprediksi Audio",
    "section": "",
    "text": "3 DATASET\nSinyal audio emosi adalah data audio yang mengandung informasi tentang ekspresi emosi seseorang yang terdengar melalui suara, seperti intonasi suara, tempo, kecepatan bicara, pitch, dan berbagai karakteristik lainnya. Dalam konteks mata kuliah proyek sains data, analisis sinyal audio emosi adalah salah satu aplikasi penting dari pemrosesan sinyal digital dan pemelajaran mesin untuk memahami dan menggambarkan emosi dalam data suara.\nPembagian data (split data) dalam konteks sains data atau pembelajaran mesin penting untuk melatih, menguji, dan mengukur kinerja model. Data dibagi menjadi set pelatihan, dan pengujian. Set pelatihan digunakan untuk melatih model, sementara set pengujian memberikan estimasi kinerja model pada data yang belum pernah dilihat sebelumnya.\nPembagian ini membantu mencegah model hanya menghafal data pelatihan dan memastikan kemampuannya dalam menggeneralisasi pola pada data baru.\nPembagian data juga berguna untuk menguji hipotesis dan memastikan bahwa evaluasi kinerja model tidak terpengaruh oleh data yang telah dilihat sebelumnya. Hal ini sangat penting dalam menghasilkan model yang dapat diandalkan dan berguna untuk mengambil keputusan pada data baru. Dengan memisahkan data dengan hati-hati, kita dapat meningkatkan validitas dan keandalan model sains data atau pembelajaran mesin yang dikembangkan.\n# Baca data dari file CSV\ndataknn= pd.read_csv('Audiofeatures.csv')\n# Pisahkan fitur (X) dan label (y)\nX = dataknn.drop(['Label','File'], axis=1)  # Ganti 'target_column' dengan nama kolom target\ny = dataknn['Label']\n\n# split data into train and test sets\nX_train,X_test,y_train, y_test= train_test_split(X, y, random_state=1, test_size=0.2)"
  },
  {
    "objectID": "Audio.html#penjelasan-dataset",
    "href": "Audio.html#penjelasan-dataset",
    "title": "2¬† Memprediksi Audio",
    "section": "3.1 PENJELASAN DATASET",
    "text": "3.1 PENJELASAN DATASET\nDataset audio ini terdapat 200 kata target yang diucapkan dalam frasa oleh dua aktris (berusia 26 dan 64 tahun). Rekaman dibuat dari set tersebut yang menggambarkan tujuh emosi (marah, jijik, takut, bahagia, kejutan, menyenangkan, kesedian dan netral).\nDataset ini terdiri dari 2746 data dengan 14 kelas dimana dua aktor merekan masing-masing 7 emosi.\nData set ini adalah data set sinyal audio yang telah dilakukan perhitungan statistika. Di mana data yang digunakan sebanyak 2810. Terdapat 10 fitur dalam perhitungan data ini, diantaranya yaitu ZCR Mean, ZCR Median, ZCR Standar Deviasi, ZCR Kurtosis, ZCR Skewness, RMSE, RMSE Median, RMSE Standar Deviasi, RMSE Kurtosis, dan RMSE Skewness."
  },
  {
    "objectID": "Audio.html#fitur",
    "href": "Audio.html#fitur",
    "title": "2¬† Memprediksi Audio",
    "section": "3.2 FITUR",
    "text": "3.2 FITUR\nPada dataset ini menjadikan perhitungan statistika sebagai fitur, dimana fitur-fitur tersebut terdiri sebanyak 10 fitur yaitu:\n\nzcr_mean\nzcr_median\nzcr_std_dev\nzcr_kurtosis\nzcr_skew\nrmse\nrmse_median\nrmse_std_dev\nrmse_kurtosis\nrmse_skew"
  },
  {
    "objectID": "Audio.html#sumber-dataset",
    "href": "Audio.html#sumber-dataset",
    "title": "2¬† Memprediksi Audio",
    "section": "3.3 SUMBER DATASET",
    "text": "3.3 SUMBER DATASET\nDataset Audio ini dapat dilihat di kaggle melalui link di bawah ini:\nhttps://www.kaggle.com/datasets/ejlok1/toronto-emotional-speech-set-tess?resource=download\n\nfolders=['YAF_sad','YAF_pleasant_surprised','YAF_neutral',\n         'YAF_happy','YAF_fear','YAF_disgust','YAF_angry',\n         'OAF_Sad','OAF_Pleasant_surprise','OAF_neutral',\n         'OAF_happy','OAF_Fear','OAF_disgust',\n         'OAF_angry',\n         ]\n\n\n# Import library yang dibutuhkan\nimport os\nimport librosa\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import skew, kurtosis, mode\n\n# Fungsi untuk menghitung statistik audio\ndef calculate_statistics(audio_path):\n    # Membaca file audio menggunakan librosa\n    y, sr = librosa.load(audio_path)\n\n    # UNTUK MENGHITUNG NILAI STATISTIKA\n    mean = np.mean(y) # Rata-rata\n    std_dev = np.std(y) # Standar deviasi\n    max_value = np.max(y) # Nilai maksimum\n    min_value = np.min(y) # Nilai minimum\n    median = np.median(y) # Nilai tengah\n    skewness = skew(y)  # Menghitung skewness\n    kurt = kurtosis(y)  # Menghitung kurtosis\n    q1 = np.percentile(y, 25)  # Kuartil pertama\n    q3 = np.percentile(y, 75)  # Kuartil ketiga\n    mode_value, _ = mode(y)  # Menghitung mode\n    iqr = q3 - q1  # Rentang interkuartil\n\n    # UNTUK MENGHITUNG NILAI ZCR (Zero Crossing Rate)\n    zcr_mean = np.mean(librosa.feature.zero_crossing_rate(y=y))\n    zcr_median = np.median(librosa.feature.zero_crossing_rate(y=y))\n    zcr_std_dev = np.std(librosa.feature.zero_crossing_rate(y=y))\n    zcr_kurtosis = kurtosis(librosa.feature.zero_crossing_rate(y=y)[0])\n    zcr_skew = skew(librosa.feature.zero_crossing_rate(y=y)[0])\n\n    # UNTUK MENGHITUNG NILAI RMSE (Root Mean Square Error)\n    rmse = np.sum(y**2) / len(y)\n    rmse_median = np.median(y**2)\n    rmse_std_dev = np.std(y**2)\n    rmse_kurtosis = kurtosis(y**2)\n    rmse_skew = skew(y**2)\n\n    return [zcr_mean, zcr_median, zcr_std_dev, zcr_kurtosis, zcr_skew, rmse, rmse_median, rmse_std_dev, rmse_kurtosis, rmse_skew]\n\n# Inisialisasi list untuk menyimpan fitur-fitur\nfeatures = []\n\n# Loop melalui folder dan file audio\nfor folder in folders:\n    folder_path = f'{folder}'\n    for filename in os.listdir(folder_path):\n        if filename.endswith('.wav'):\n            audio_path = os.path.join(folder_path, filename)\n            # Menghitung statistik audio menggunakan fungsi yang telah dibuat\n            statistics = calculate_statistics(audio_path)\n            # Menambahkan label folder dan nama file ke fitur-fitur\n            features.append([folder, filename] + statistics)\n\n# Membuat DataFrame dari data fitur\ncolumns =  ['Label', 'File'] + ['ZCR Mean', 'ZCR Median', 'ZCR Std Dev', 'ZCR Kurtosis', 'ZCR Skew', 'RMSE', 'RMSE Median', 'RMSE Std Dev', 'RMSE Kurtosis', 'RMSE Skew']\ndf = pd.DataFrame(features, columns=columns)\n\n# Menampilkan DataFrame sebagai file CSV\ndf\n\n\n  \n    \n\n\n\n\n\n\nLabel\nFile\nZCR Mean\nZCR Median\nZCR Std Dev\nZCR Kurtosis\nZCR Skew\nRMSE\nRMSE Median\nRMSE Std Dev\nRMSE Kurtosis\nRMSE Skew\n\n\n\n\n0\nYAF_sad\nYAF_sour_sad.wav\n0.167885\n0.042480\n0.239094\n0.742978\n1.544715\n0.001091\n0.000271\n0.002257\n47.882266\n5.145248\n\n\n1\nYAF_sad\nYAF_red_sad.wav\n0.117450\n0.035156\n0.206319\n4.820082\n2.492407\n0.001321\n0.000218\n0.002913\n31.518783\n4.611515\n\n\n2\nYAF_sad\nYAF_seize_sad.wav\n0.209233\n0.041016\n0.272344\n-0.418738\n1.134397\n0.001557\n0.000298\n0.003292\n26.712456\n4.404659\n\n\n3\nYAF_sad\nYAF_team_sad.wav\n0.140697\n0.040039\n0.220488\n2.793284\n2.032850\n0.001886\n0.000391\n0.003773\n23.012555\n4.083049\n\n\n4\nYAF_sad\nYAF_moon_sad.wav\n0.097233\n0.035156\n0.187809\n5.664555\n2.681021\n0.003097\n0.000750\n0.005715\n13.519917\n3.295909\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2527\nOAF_disgust\nOAF_jar_disgust.wav\n0.114746\n0.046387\n0.163155\n2.891574\n2.075589\n0.000478\n0.000090\n0.001238\n55.841742\n6.312788\n\n\n2528\nOAF_disgust\nOAF_judge_disgust.wav\n0.109677\n0.045654\n0.159212\n4.190305\n2.328250\n0.000934\n0.000108\n0.003004\n59.761747\n6.782871\n\n\n2529\nOAF_disgust\nOAF_keep_disgust.wav\n0.118263\n0.036865\n0.178795\n1.508613\n1.742521\n0.000375\n0.000031\n0.001087\n45.891859\n5.881692\n\n\n2530\nOAF_disgust\nOAF_jug_disgust.wav\n0.117042\n0.041992\n0.155269\n1.017243\n1.618774\n0.001240\n0.000135\n0.003419\n44.232652\n5.770736\n\n\n2531\nOAF_disgust\nOAF_hush_disgust.wav\n0.137700\n0.063232\n0.160404\n1.894541\n1.768188\n0.000314\n0.000020\n0.001046\n57.276662\n6.652799\n\n\n\n\n\n2532 rows √ó 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Menyimpannya ke CSV\ndf.to_csv('Audiofeatures.csv',index=False)"
  },
  {
    "objectID": "Audio.html#normalisasi",
    "href": "Audio.html#normalisasi",
    "title": "2¬† Memprediksi Audio",
    "section": "5.1 Normalisasi",
    "text": "5.1 Normalisasi\nNormalisasi adalah proses mengubah nilai-nilai dalam suatu dataset sehingga dapat memiliki skala yang seragam. Tujuannya adalah untuk memastikan bahwa variabel-variabel dengan rentang nilai yang berbeda-beda memiliki dampak yang setara terhadap analisis atau pembelajaran mesin yang dilakukan. Normalisasi seringkali diterapkan pada data numerik sebelum digunakan dalam model pembelajaran mesin atau analisis statistik seperti dibawah ini\n\n# define scaler\nscaler = StandardScaler()\n# fit scaler on the training dataset\nscaler.fit(X_train)\n# save the scaler\ndump(scaler, open('scaler.pkl', 'wb'))\n# transform the training dataset\nX_train_scaled = scaler.transform(X_train)\n\ndataknn\n\n\n  \n    \n\n\n\n\n\n\nLabel\nFile\nZCR Mean\nZCR Median\nZCR Std Dev\nZCR Kurtosis\nZCR Skew\nRMSE\nRMSE Median\nRMSE Std Dev\nRMSE Kurtosis\nRMSE Skew\n\n\n\n\n0\nYAF_sad\nYAF_sour_sad.wav\n0.167885\n0.042480\n0.239094\n0.742978\n1.544715\n0.001091\n0.000271\n0.002257\n47.882266\n5.145248\n\n\n1\nYAF_sad\nYAF_red_sad.wav\n0.117450\n0.035156\n0.206319\n4.820082\n2.492407\n0.001321\n0.000218\n0.002913\n31.518783\n4.611515\n\n\n2\nYAF_sad\nYAF_seize_sad.wav\n0.209233\n0.041016\n0.272344\n-0.418738\n1.134397\n0.001557\n0.000298\n0.003292\n26.712456\n4.404659\n\n\n3\nYAF_sad\nYAF_team_sad.wav\n0.140697\n0.040039\n0.220488\n2.793284\n2.032850\n0.001886\n0.000391\n0.003773\n23.012555\n4.083049\n\n\n4\nYAF_sad\nYAF_moon_sad.wav\n0.097233\n0.035156\n0.187809\n5.664555\n2.681021\n0.003097\n0.000750\n0.005715\n13.519917\n3.295909\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2527\nOAF_disgust\nOAF_jar_disgust.wav\n0.114746\n0.046387\n0.163155\n2.891574\n2.075589\n0.000478\n0.000090\n0.001238\n55.841742\n6.312788\n\n\n2528\nOAF_disgust\nOAF_judge_disgust.wav\n0.109677\n0.045654\n0.159212\n4.190305\n2.328250\n0.000934\n0.000108\n0.003004\n59.761747\n6.782871\n\n\n2529\nOAF_disgust\nOAF_keep_disgust.wav\n0.118263\n0.036865\n0.178795\n1.508613\n1.742521\n0.000375\n0.000031\n0.001087\n45.891859\n5.881692\n\n\n2530\nOAF_disgust\nOAF_jug_disgust.wav\n0.117042\n0.041992\n0.155269\n1.017243\n1.618774\n0.001240\n0.000135\n0.003419\n44.232652\n5.770736\n\n\n2531\nOAF_disgust\nOAF_hush_disgust.wav\n0.137700\n0.063232\n0.160404\n1.894541\n1.768188\n0.000314\n0.000020\n0.001046\n57.276662\n6.652799\n\n\n\n\n\n2532 rows √ó 12 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n# Membuka file pickle\nwith open('scaler.pkl', 'rb') as standarisasi:\n    loadscal= pickle.load(standarisasi)\n\nX_test_scaled=loadscal.transform(X_test) #normalisasi X testing dari hasil normalisasi X train yang disimpan dalam model"
  },
  {
    "objectID": "Audio.html#penjelasan-k-nn",
    "href": "Audio.html#penjelasan-k-nn",
    "title": "2¬† Memprediksi Audio",
    "section": "6.1 Penjelasan K-NN",
    "text": "6.1 Penjelasan K-NN\nK-NN, atau K-Nearest Neighbors, adalah algoritma pembelajaran mesin yang digunakan untuk klasifikasi dan regresi. Ide dasar di balik K-NN adalah mengklasifikasikan atau memprediksi suatu data berdasarkan mayoritas label (untuk klasifikasi) atau nilai rata-rata (untuk regresi) dari K tetangga terdekatnya di dalam ruang fitur. Artinya, sebuah data akan diatributkan label atau nilai yang paling umum atau rata-rata di antara tetangga-tetangganya."
  },
  {
    "objectID": "Audio.html#langkah-langkah-k-nn",
    "href": "Audio.html#langkah-langkah-k-nn",
    "title": "2¬† Memprediksi Audio",
    "section": "6.2 Langkah-Langkah K-NN:",
    "text": "6.2 Langkah-Langkah K-NN:\n\nPemilihan Jumlah Tetangga (K): &gt; Pilih suatu nilai K, yang merupakan jumlah tetangga terdekat yang akan digunakan untuk membuat prediksi.\nMenghitung Jarak: &gt; Hitung jarak antara data yang akan diprediksi dengan semua data lain dalam dataset menggunakan suatu metrik jarak, seperti Euclidean distance atau Manhattan distance.\nIdentifikasi Tetangga Terdekat: &gt; Pilih K tetangga terdekat berdasarkan nilai jarak yang dihitung.\nKlasifikasi atau Regresi: &gt; Untuk klasifikasi, atributkan label yang paling umum di antara tetangga-tetangga tersebut kepada data yang akan diprediksi. Untuk regresi, atributkan nilai rata-rata dari nilai target tetangga-tetangga tersebut kepada data yang akan diprediksi."
  },
  {
    "objectID": "Audio.html#rumus-perhitungan-jarak-euclidean",
    "href": "Audio.html#rumus-perhitungan-jarak-euclidean",
    "title": "2¬† Memprediksi Audio",
    "section": "6.3 Rumus Perhitungan Jarak Euclidean",
    "text": "6.3 Rumus Perhitungan Jarak Euclidean\nDalam kasus klasifikasi dengan menggunakan jarak Euclidean, rumus perhitungan jarak antara dua titik \\(P(x_1,y_1)\\) dan \\(Q(x_2,y_2)\\) pada bidang dua dimensi adalah:\n\\(Euclidean\\) \\(Distance =\\) \\(\\sqrt{(x_2-x_1)^2+(y_2-y_2)^2}\\)\n\n# Jumlah tetangga (neighbors) yang akan diuji\nK = 10\n\n# Inisialisasi array untuk menyimpan akurasi\nacc = np.zeros((K-1))\n\n# Melakukan iterasi untuk nilai n_neighbors dari 1 hingga K dengan langkah 2\nfor n in range(1, K, 2):\n    # Membuat model KNN dengan jumlah tetangga n dan metrik Euclidean\n    knn = KNeighborsClassifier(n_neighbors=n, metric=\"euclidean\").fit(X_train_scaled, y_train)\n\n    # Melakukan prediksi pada data uji\n    y_pred = knn.predict(X_test_scaled)\n\n    # Menghitung akurasi dan menyimpannya dalam array acc\n    acc[n-1] = accuracy_score(y_test, y_pred)\n\n# Menampilkan hasil akurasi terbaik dan nilai k yang sesuai\nprint('Akurasi terbaik adalah', acc.max(), 'dengan nilai k =', acc.argmax()+1)\n\nAkurasi terbaik adalah 0.6844181459566075 dengan nilai k = 5\n\n\n\n# Inisialisasi model KNeighborsClassifier dengan 13 tetangga terdekat (n_neighbors=13)\n# dan menggunakan metrik jarak \"euclidean\" untuk mengukur jarak antar data.\nknn = KNeighborsClassifier(n_neighbors=13, metric=\"euclidean\")\n\n# Simpan model KNeighborsClassifier ke dalam file 'modelknn.pkl' menggunakan fungsi dump dari joblib.\ndump(knn, open('modelknn.pkl', 'wb'))\n\n\n# Membuka file 'modelknn.pkl' dalam mode pembacaan biner ('rb')\nwith open('modelknn.pkl', 'rb') as knn:\n    # Melakukan deserialisasi objek K-Nearest Neighbors (KNN) dari file\n    loadknn = pickle.load(knn)\n\n# Melakukan pelatihan (fitting) kembali pada model KNN yang telah di-deserialize\n# dengan menggunakan data yang telah di-scaling (X_train_scaled) dan labelnya (y_train)\nloadknn.fit(X_train_scaled, y_train)\n\nKNeighborsClassifier(metric='euclidean', n_neighbors=13)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(metric='euclidean', n_neighbors=13)\n\n\n\ny_pred = loadknn.predict(X_test_scaled)\ny_pred\n\narray(['YAF_disgust', 'OAF_Fear', 'OAF_disgust', 'OAF_Sad', 'OAF_Fear',\n       'YAF_happy', 'YAF_happy', 'OAF_Pleasant_surprise', 'YAF_neutral',\n       'YAF_neutral', 'YAF_happy', 'YAF_sad', 'OAF_Sad',\n       'OAF_Pleasant_surprise', 'YAF_fear', 'YAF_neutral', 'YAF_angry',\n       'YAF_pleasant_surprised', 'YAF_disgust', 'YAF_neutral',\n       'YAF_pleasant_surprised', 'YAF_pleasant_surprised', 'YAF_fear',\n       'YAF_happy', 'YAF_pleasant_surprised', 'OAF_Fear', 'OAF_happy',\n       'YAF_sad', 'YAF_sad', 'YAF_happy', 'YAF_neutral', 'YAF_sad',\n       'YAF_disgust', 'YAF_pleasant_surprised', 'OAF_neutral',\n       'OAF_neutral', 'OAF_neutral', 'YAF_sad', 'OAF_happy', 'YAF_happy',\n       'YAF_angry', 'YAF_happy', 'YAF_neutral', 'OAF_happy',\n       'OAF_disgust', 'OAF_Fear', 'OAF_Fear', 'YAF_neutral', 'YAF_happy',\n       'OAF_happy', 'OAF_Sad', 'YAF_disgust', 'OAF_Pleasant_surprise',\n       'YAF_happy', 'YAF_angry', 'OAF_happy', 'OAF_Fear', 'YAF_fear',\n       'YAF_disgust', 'OAF_Fear', 'OAF_happy', 'OAF_Sad', 'OAF_happy',\n       'YAF_happy', 'OAF_Fear', 'YAF_pleasant_surprised',\n       'YAF_pleasant_surprised', 'OAF_Pleasant_surprise', 'YAF_happy',\n       'OAF_Fear', 'YAF_disgust', 'OAF_neutral', 'YAF_disgust',\n       'OAF_disgust', 'YAF_neutral', 'OAF_Fear', 'OAF_Sad', 'YAF_happy',\n       'YAF_happy', 'YAF_happy', 'YAF_angry', 'YAF_happy', 'OAF_disgust',\n       'YAF_neutral', 'OAF_Pleasant_surprise', 'YAF_angry', 'YAF_sad',\n       'OAF_Fear', 'YAF_pleasant_surprised', 'YAF_neutral', 'YAF_fear',\n       'OAF_Pleasant_surprise', 'OAF_happy', 'OAF_happy', 'OAF_Sad',\n       'OAF_neutral', 'YAF_disgust', 'YAF_neutral', 'YAF_fear',\n       'OAF_happy', 'YAF_happy', 'YAF_angry', 'OAF_disgust', 'YAF_sad',\n       'OAF_Pleasant_surprise', 'YAF_fear', 'YAF_happy', 'OAF_Sad',\n       'YAF_pleasant_surprised', 'OAF_Fear', 'YAF_happy', 'OAF_Fear',\n       'OAF_neutral', 'YAF_fear', 'YAF_pleasant_surprised', 'OAF_happy',\n       'OAF_Pleasant_surprise', 'YAF_neutral', 'YAF_angry',\n       'YAF_pleasant_surprised', 'YAF_sad', 'OAF_happy', 'YAF_angry',\n       'OAF_disgust', 'OAF_Sad', 'OAF_Pleasant_surprise', 'YAF_angry',\n       'YAF_angry', 'OAF_neutral', 'OAF_neutral', 'OAF_happy',\n       'YAF_pleasant_surprised', 'OAF_happy', 'OAF_happy',\n       'YAF_pleasant_surprised', 'OAF_neutral', 'OAF_Sad', 'OAF_Sad',\n       'OAF_Pleasant_surprise', 'OAF_disgust', 'OAF_Pleasant_surprise',\n       'YAF_disgust', 'YAF_angry', 'YAF_fear', 'OAF_Sad', 'YAF_happy',\n       'OAF_neutral', 'YAF_neutral', 'YAF_disgust', 'YAF_disgust',\n       'YAF_disgust', 'OAF_Fear', 'YAF_angry', 'YAF_happy',\n       'YAF_pleasant_surprised', 'OAF_disgust', 'YAF_pleasant_surprised',\n       'YAF_pleasant_surprised', 'OAF_Pleasant_surprise', 'OAF_Sad',\n       'OAF_Fear', 'YAF_sad', 'YAF_angry', 'YAF_happy', 'YAF_sad',\n       'YAF_angry', 'OAF_happy', 'YAF_disgust', 'YAF_neutral', 'OAF_Sad',\n       'YAF_neutral', 'YAF_angry', 'OAF_Sad', 'OAF_Sad',\n       'YAF_pleasant_surprised', 'OAF_neutral', 'YAF_disgust',\n       'OAF_Pleasant_surprise', 'OAF_Pleasant_surprise',\n       'OAF_Pleasant_surprise', 'OAF_neutral', 'OAF_happy',\n       'OAF_Pleasant_surprise', 'OAF_disgust', 'YAF_angry',\n       'YAF_pleasant_surprised', 'YAF_happy', 'YAF_angry', 'OAF_Fear',\n       'OAF_neutral', 'YAF_sad', 'YAF_sad', 'OAF_neutral', 'OAF_happy',\n       'YAF_happy', 'YAF_angry', 'YAF_disgust', 'OAF_neutral',\n       'OAF_neutral', 'OAF_Fear', 'OAF_disgust', 'OAF_happy',\n       'OAF_Pleasant_surprise', 'YAF_neutral', 'OAF_Pleasant_surprise',\n       'OAF_Sad', 'YAF_pleasant_surprised', 'YAF_angry', 'YAF_angry',\n       'OAF_happy', 'YAF_disgust', 'YAF_happy', 'YAF_happy',\n       'YAF_disgust', 'OAF_neutral', 'OAF_Fear', 'OAF_Fear',\n       'YAF_disgust', 'OAF_Pleasant_surprise', 'YAF_sad', 'YAF_fear',\n       'YAF_neutral', 'YAF_pleasant_surprised', 'YAF_pleasant_surprised',\n       'OAF_Fear', 'YAF_angry', 'YAF_sad', 'YAF_fear', 'YAF_sad',\n       'YAF_pleasant_surprised', 'OAF_Pleasant_surprise', 'OAF_happy',\n       'YAF_disgust', 'YAF_happy', 'YAF_fear', 'YAF_neutral',\n       'OAF_neutral', 'OAF_Fear', 'OAF_happy', 'YAF_neutral',\n       'OAF_Pleasant_surprise', 'YAF_neutral', 'YAF_happy', 'OAF_Sad',\n       'YAF_happy', 'OAF_Sad', 'YAF_disgust', 'OAF_Pleasant_surprise',\n       'YAF_angry', 'YAF_angry', 'OAF_happy', 'YAF_angry', 'YAF_sad',\n       'OAF_happy', 'YAF_pleasant_surprised', 'OAF_disgust', 'OAF_happy',\n       'YAF_sad', 'YAF_angry', 'YAF_neutral', 'YAF_disgust', 'YAF_happy',\n       'YAF_happy', 'OAF_happy', 'YAF_happy', 'YAF_happy',\n       'OAF_Pleasant_surprise', 'YAF_neutral', 'OAF_Fear',\n       'OAF_Pleasant_surprise', 'YAF_fear', 'YAF_fear', 'OAF_Fear',\n       'OAF_neutral', 'OAF_disgust', 'YAF_disgust', 'OAF_neutral',\n       'YAF_angry', 'OAF_neutral', 'YAF_disgust', 'YAF_sad', 'YAF_angry',\n       'OAF_happy', 'YAF_angry', 'OAF_Sad', 'OAF_disgust', 'YAF_sad',\n       'YAF_pleasant_surprised', 'OAF_happy', 'YAF_neutral', 'OAF_Sad',\n       'YAF_pleasant_surprised', 'YAF_angry', 'YAF_neutral', 'YAF_angry',\n       'OAF_happy', 'YAF_fear', 'YAF_neutral', 'YAF_angry',\n       'YAF_pleasant_surprised', 'YAF_neutral', 'OAF_happy',\n       'YAF_disgust', 'YAF_sad', 'YAF_disgust', 'YAF_happy', 'YAF_sad',\n       'OAF_Fear', 'OAF_Pleasant_surprise', 'YAF_angry', 'YAF_neutral',\n       'OAF_happy', 'OAF_happy', 'YAF_pleasant_surprised',\n       'YAF_pleasant_surprised', 'OAF_happy', 'YAF_neutral', 'YAF_fear',\n       'OAF_neutral', 'YAF_pleasant_surprised', 'OAF_Sad', 'OAF_Sad',\n       'YAF_angry', 'YAF_angry', 'OAF_happy', 'YAF_happy', 'YAF_happy',\n       'OAF_Sad', 'YAF_fear', 'YAF_angry', 'OAF_Sad', 'YAF_neutral',\n       'OAF_neutral', 'YAF_disgust', 'YAF_neutral', 'YAF_neutral',\n       'OAF_neutral', 'OAF_happy', 'YAF_pleasant_surprised', 'OAF_Fear',\n       'YAF_disgust', 'YAF_happy', 'OAF_happy', 'YAF_disgust',\n       'OAF_Pleasant_surprise', 'YAF_fear', 'YAF_happy', 'YAF_angry',\n       'YAF_sad', 'OAF_Pleasant_surprise', 'YAF_fear', 'OAF_neutral',\n       'OAF_neutral', 'OAF_happy', 'OAF_Sad', 'YAF_angry', 'OAF_Sad',\n       'YAF_happy', 'YAF_happy', 'YAF_fear', 'OAF_neutral',\n       'YAF_pleasant_surprised', 'YAF_neutral', 'OAF_happy',\n       'OAF_neutral', 'OAF_happy', 'YAF_angry', 'OAF_Fear', 'OAF_neutral',\n       'OAF_neutral', 'YAF_pleasant_surprised', 'OAF_happy', 'YAF_angry',\n       'OAF_Sad', 'YAF_happy', 'OAF_Fear', 'OAF_neutral', 'OAF_neutral',\n       'OAF_disgust', 'YAF_disgust', 'OAF_disgust', 'OAF_happy',\n       'OAF_Sad', 'OAF_happy', 'YAF_fear', 'YAF_happy', 'OAF_happy',\n       'OAF_happy', 'OAF_neutral', 'YAF_happy', 'OAF_happy', 'YAF_angry',\n       'YAF_pleasant_surprised', 'OAF_happy', 'YAF_happy', 'YAF_fear',\n       'OAF_Pleasant_surprise', 'YAF_neutral', 'OAF_happy', 'YAF_neutral',\n       'YAF_happy', 'OAF_happy', 'YAF_pleasant_surprised', 'OAF_happy',\n       'YAF_angry', 'OAF_Sad', 'OAF_Sad', 'YAF_pleasant_surprised',\n       'OAF_happy', 'YAF_happy', 'YAF_angry', 'OAF_Sad', 'YAF_fear',\n       'OAF_disgust', 'YAF_neutral', 'YAF_fear', 'OAF_neutral',\n       'YAF_disgust', 'OAF_Pleasant_surprise', 'YAF_disgust',\n       'OAF_neutral', 'YAF_angry', 'YAF_happy', 'YAF_happy', 'YAF_fear',\n       'YAF_pleasant_surprised', 'OAF_happy', 'OAF_Fear', 'OAF_happy',\n       'YAF_disgust', 'OAF_disgust', 'YAF_sad', 'YAF_angry', 'YAF_angry',\n       'OAF_Fear', 'YAF_angry', 'OAF_happy', 'YAF_sad', 'OAF_happy',\n       'YAF_pleasant_surprised', 'YAF_fear', 'YAF_neutral', 'OAF_neutral',\n       'YAF_sad', 'OAF_Sad', 'YAF_angry', 'OAF_neutral', 'YAF_disgust',\n       'YAF_angry', 'OAF_disgust', 'YAF_sad', 'YAF_angry', 'YAF_happy',\n       'OAF_disgust', 'YAF_neutral', 'OAF_Sad', 'YAF_neutral', 'YAF_sad',\n       'OAF_neutral', 'YAF_sad', 'OAF_Sad', 'YAF_pleasant_surprised',\n       'OAF_disgust', 'YAF_disgust', 'YAF_angry', 'OAF_Fear',\n       'YAF_disgust', 'OAF_Fear', 'OAF_happy', 'YAF_happy', 'YAF_angry',\n       'OAF_Sad', 'OAF_neutral', 'YAF_pleasant_surprised', 'OAF_Sad',\n       'OAF_neutral', 'YAF_pleasant_surprised', 'OAF_disgust',\n       'OAF_happy', 'YAF_neutral', 'OAF_Fear', 'YAF_pleasant_surprised',\n       'YAF_fear', 'OAF_Pleasant_surprise', 'OAF_happy', 'OAF_Sad',\n       'YAF_angry', 'YAF_angry', 'OAF_Fear', 'OAF_happy', 'YAF_fear',\n       'YAF_neutral', 'YAF_pleasant_surprised', 'YAF_pleasant_surprised',\n       'YAF_disgust', 'YAF_neutral', 'YAF_disgust', 'OAF_Fear',\n       'OAF_neutral', 'OAF_neutral', 'YAF_happy', 'OAF_neutral',\n       'OAF_happy', 'YAF_happy', 'YAF_angry', 'OAF_Fear', 'YAF_happy'],\n      dtype=object)\n\n\n\naccuracy = accuracy_score(y_test,y_pred)\nprint(\"Akurasi:\",accuracy)\n\nAkurasi: 0.6824457593688363"
  },
  {
    "objectID": "Audio.html#reduksi-data",
    "href": "Audio.html#reduksi-data",
    "title": "2¬† Memprediksi Audio",
    "section": "6.4 REDUKSI DATA",
    "text": "6.4 REDUKSI DATA\nreduksi ada 2 jenis bisa menggunakan seleksi data atau transformation data(contoh nya PCA / Principal component anlysis). seleksi data kita dapat emmilih fitur berdasarkan yang fitur/ kolom yang paling berpengaruh. sedangkan transformasi data kita perlu membuat kooordinat baru dari dari fitur yang ada sejumlah dengan jumlah fitur nya. mengapa reduksi data atau data reduction di perlukan? karna terlalu banyak kolom/fitur/ciri yang harus dikenali tidak baik untuk pemprosesan data dan memakan waktu komputasi yang lama, maka dari itu reduksi data di lakukan untuk mendapatkan data yang terbaik. ketika ingin mencari koordinat baru:\nbuat matriks covarian buat persamaan\nnote : konstanta yang paling besar mengartikan di koordinat tersebut merupakan ciri paling banyak atau penting\n\n# Mengimpor modul PCA (Principal Component Analysis) dari pustaka scikit-learn\nfrom sklearn.decomposition import PCA as sklearnPCA\n\n# Membuat objek PCA dengan 8 komponen\nsklearn_pca = sklearnPCA(n_components=8)\n\n# Melakukan transformasi PCA pada data latihan yang telah dinormalisasi\nX_train_pca = sklearn_pca.fit_transform(X_train_scaled)\n\n# Menyimpan objek PCA ke dalam file 'PCA8.pkl' menggunakan modul pickle\ndump(sklearn_pca, open('PCA8.pkl', 'wb'))\n\n# Membuka kembali file 'PCA8.pkl' untuk mendapatkan objek PCA yang telah disimpan\nwith open('PCA8.pkl', 'rb') as pca:\n    loadpca = pickle.load(pca)\n\n# Melakukan transformasi PCA pada data uji yang telah dinormalisasi\nX_test_pca = loadpca.transform(X_test_scaled)\nX_test_pca.shape\n\n(507, 8)\n\n\n\n# Mengimpor modul KNeighborsClassifier dari scikit-learn\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Membuat objek klasifikasi K-Nearest Neighbors dengan jumlah tetangga sebanyak 15\nclassifier = KNeighborsClassifier(n_neighbors=15)\n\n# Melatih model klasifikasi menggunakan data latihan yang telah direduksi dimensinya dengan PCA\nclassifier.fit(X_train_pca, y_train)\n\n# Melakukan prediksi menggunakan data uji yang telah direduksi dimensinya dengan PCA\ny_prediksi = classifier.predict(X_test_pca)\n\n# Mengimpor modul accuracy_score dari scikit-learn\nfrom sklearn.metrics import accuracy_score\n\n# Menghitung akurasi dari prediksi yang telah dilakukan\nacc_pca = accuracy_score(y_test, y_prediksi)\n\n# Menampilkan akurasi hasil prediksi\nprint(\"Akurasi:\", acc_pca)\n\nAkurasi: 0.6706114398422091"
  }
]